# Lip-to-Speech Synthesis

The lip-to-speech synthesis technology for non-English languages remains relatively unexplored within the current scientific literature. This project will build upon previous works to produce a model that is able to synthesize speech based on lip motions of a speaker for multiple languages, not just English.  By doing so, cross-language communication can be improved upon.

## Acknowledgements
This repository builds upon and contains the PyTorch implementation of the following paper:
> **Lip to Speech Synthesis with Visual Context Attentional GAN**<br>
> Minsu Kim, Joanna Hong, and Yong Man Ro<br>
> \[[Paper](https://proceedings.neurips.cc/paper/2021/file/16437d40c29a1a7b1e78143c9c38f289-Paper.pdf)\]
